---
title: "CTA_wk3_dictionary"
author: "Marion Lieutaud"
date: "1/31/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
```{r packages}
library(tidyverse)
library(quanteda)
library(readtext)
install.packages("tm")
library("tm")
library(tidytext)
library(stringi) #to generate random text
library(dplyr) #tidyverse package for wrangling data
library(ggplot2) #package for visualizing data
library(scales) #additional package for formatting plot axes
library(kableExtra) #package for displaying data in html format (relevant for formatting this worksheet mainly)
install.packages("quanteda")
```

## Data
\textcolor{red}{The most common problem related to loading data into R are misspecified locations of files or directories.}

If a *path* is relative, check where you are using getwd() and set the root directory of your project using setwd(). On Windows, you also have to replace all \ in a path with /.

You can also use the R menu at the top of your screen: Session / Set working directory

```{r}
getwd()
```

## import the barbie and oppenheimer data from the Github repo
use read.csv() command
```{r}
barbie_posts <- read.csv("/Users/liuruisi/Desktop/ComputationalTextAnalysis_Week03/Barbie_Reddit_Posts - Barbie_Reddit_Posts.csv")
oppenheimer_posts <- read.csv("/Users/liuruisi/Desktop/ComputationalTextAnalysis_Week03/Oppenheimer_Reddit_Posts - Oppenheimer_Reddit_Posts.csv")
```

## bind the different datasets
```{r}
# create variable to identify the two datasets
# take barbie dataset, create a new barbie dataset, and the initial dataset wouldn't have it
barbie_posts <- barbie_posts %>%
  mutate(movie = "barbie")
View(barbie_posts)

oppenheimer_posts <- oppenheimer_posts %>%
  mutate(movie = "oppenheimer")
View(oppenheimer_posts)
# bind rows to make a single dataset
barbie_oppenheimer_posts <- rbind(barbie_posts, oppenheimer_posts)

# check the movie variable
frequency(barbie_oppenheimer_posts$movie)
table(barbie_oppenheimer_posts$movie)
view(barbie_oppenheimer_posts)

# add row number variable
barbie_oppenheimer_posts <- barbie_oppenheimer_posts %>%
  as.data.frame()
barbie_oppenheimer_posts <- barbie_oppenheimer_posts %>%
  mutate(row_number = row_number())
barbie_oppenheimer_posts <- barbie_oppenheimer_posts %>%
  select(row_number, everything())
view(barbie_oppenheimer_posts)
```
note: binding is a way of merging data (there are other ways)


# Basic operations and preprocessing

## corpus
```{r}
#creating corpus
barbie_oppenheimer_posts <- barbie_oppenheimer_posts %>%
  mutate(Combined.Text = paste(Post.Title, Post.Text, sep = "-"))
barbie_oppenheimer_posts <- barbie_oppenheimer_posts %>%
  as.data.frame()

row.names(barbie_oppenheimer_posts) <- make.names(seq_len(nrow(barbie_oppenheimer_posts)), unique = TRUE)
docvars_df <- data.frame(Post.Date = barbie_oppenheimer_posts$Post.Date)
rm(corpus_bo)
barbie_oppenheimer_posts <- barbie_oppenheimer_posts
corpus_bo <- corpus(barbie_oppenheimer_posts, 
                    text_field = "Combined.Text")
view(corpus_bo)

#assigning names to each document
docid <- paste(barbie_oppenheimer_posts$movie,
               barbie_oppenheimer_posts$row_number, sep = "_")
docnames(corpus_bo) <- docid
print(corpus_bo)
```

```{r}
# subsetting corpus
num_doc <- ndoc(corpus_bo)
print(num_doc)
head(docvars(corpus_bo))

# extracting document-level variables
docvars(corpus_bo, field = "Post.Text")
```

## Tokenisation and cleanup
```{r}
# start from corpus data
# remove punctuation
# remove stopwords
# to lower but keep acronyms
tokens_bo <- tokens(corpus_bo, remove_punct = TRUE)
tokens_bo <- tokens_remove(tokens_bo, stopwords("en"))
tokens_bo <- tokens_tolower(tokens_bo, keep_acronyms = TRUE)
tokens_bo
  
```

## document-feature matrix
```{r}
# you can also do a lot of text data preprocessing after creating a Dfm, e.g. 
# and you can use it to select or remove features
dfm_bo <- dfm(tokens_bo)

```

## removing features, introducing Regex

Look up regex cheatsheet
```{r}

```

# regular expressions, glob vs regex, fixed
```{r}
```

# dictionary method
```{r}
library(remotes)
Sys.setenv(GITHUB_PAT = "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")
remotes::install_github("kbenoit/quanteda.dictionaries")
library(quanteda.dictionaries)
```
Dictionary creation is done through the `dictionary()` function, which classes a named list of characters as a dictionary.

## creating your own dictionary
```{r}
# create your own dictionary
my_dictionary <- dictionary(list(
  positive = c("good", "happy", "joy", "excellent", "amazing", "love", "best", "favorite", "positive"),
  negative = c("bad", "sad", "anger", "terrible", "horrible", "complain", "negative")
))
```

The most frequent scenario is when we pass through a dictionary at the time of `dfm()` creation.
```{r}
# dfm with dictionaries
dfm_sentiment <- dfm_lookup(dfm_bo, dictionary = data_dictionary_LSD2015_pos_neg)
```


## Applying an existing dictionary
Apply the Lexicoder Sentiment Dictionary to the selected contexts using tokens_lookup().
```{r}
# look at the categories of the Lexicoder
lengths(data_dictionary_LSD2015)

# select only the "negative" and "positive" categories
data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]
```


```{r}
# go back to our barbie/oppenheimer tokenised data
# create a document document-feature matrix and group it by day
dfm_bo_grouped <- dfm_group(dfm_bo, groups = docvars(corpus_bo, "Post.Date"))
print(dfm_bo_grouped)

# prep data + sentiment ratio variable for analysis
dfm_bo_sentiment <- dfm_lookup(dfm_bo, dictionary = data_dictionary_LSD2015_pos_neg)
sentiment_ratio <- (dfm_bo_sentiment[, "positive"] - dfm_bo_sentiment[, "negative"]) / (dfm_bo_sentiment[, "positive"] + dfm_bo_sentiment[, "negative"])

# basic plot: frequency of positive words
df_sentiment <- convert(dfm_bo_sentiment, to = "data.frame")
df_sentiment$Post.Date <- docvars(corpus_bo, "Post.Date")

positive_df <- df_sentiment %>%
  group_by(Post.Date) %>%
  summarise(Frequency = sum(positive, na.rm = TRUE))

ggplot(positive_df, aes(x = as.Date(Post.Date), y = Frequency)) +
  geom_bar(stat = "identity", fill = "red") +
  xlab("Post Date") +
  ylab("Frequency of Positive Words") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  

# basic plot: frequency of positive/negative words
positive_freq <- colSums(dfm_bo_sentiment[, "positive"])
negative_freq <- colSums(dfm_bo_sentiment[, "negative"])

sentiment_df <- data.frame(
  Post.Date = names(positive_freq),
  Positive = positive_freq,
  Negative = negative_freq
)

library(tidyr)
sentiment_long <- pivot_longer(sentiment_df, cols = c("Positive", "Negative"), names_to = "Sentiment", values_to = "Frequency")

ggplot(sentiment_long, aes(x = Post.Date, y = Frequency, fill = Sentiment)) +
  geom_bar(stat = "identity", position = "dodge") +
  xlab("Post Date") +
  ylab("Frequency of Words") +
  labs(fill = "Sentiment") +
  theme_minimal()
```

